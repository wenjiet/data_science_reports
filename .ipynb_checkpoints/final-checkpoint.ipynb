{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Community: Social Discovery of Artificial Intelligence Researchers and Literatures\n",
    "\n",
    "Artificial Intelligence is one of the most influential subjects in computer science. We will analyze scholars’ activities based on literatures published in the top-tier conferences to gain some interesting insights into this field. \n",
    "\n",
    "In this report, we start by a brief introduction of the dataset and data processing. Then we present a temporal analysis of AI topic popularity. In section 3, spatial distribution of AI research is discussed. Next, in section 4, we implement a community finding algorithm on coauthorship and citation graphs, which identifies important research communities and centers within the graph. Finally, we build a simple paper recommendation system based on content similarity and citation relationship. \n",
    "\n",
    "Note: The code cannot be run directly from this Python Notebook. Because our code is too long to be included. We have included a separate folder for code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection and Processing\n",
    "For this project, our data mainly come from two public dataset, DBLP and ArnetMiner. Apart from them, we also scraped data from Google Map for geographical information. In this section, we will discuss how we parsed them and how we used them.\n",
    "\n",
    "### Data Selection\n",
    "DBLP is a well-maintained dataset of computer science bibliography. It contains 3,583,224 publications and 1,824,011 authors[1]. It covers a huge amount of computer science publications. Taking advantage of the widely covered publications, DBLP is used for analyzing the trending of topics. We also gathered and formatted the co-authorship information for the analysis on community finding. However, DBLP has two obvious drawbacks. DBLP does not have well maintained author information and it does not have the paper citation data. It causes great inconvenience for our further analysis so we turned to a second dataset ArnetMiner.\n",
    "\n",
    "The other public dataset we use is ArnetMiner[2], which is a supplement to DBLP that has well-maintained metadata including citations and author metadata. ArnetMiner contains 2,092,356 publications, 1,712,433 authors and 8,024,869 citation relationships[2]. Thus, we used the ArnetMiner dataset for community finding based on citation relationships. For the analysis of authors’ geographical distribution, we also scraped the geographical coordinates of the authors’ affiliations to locate each author.\n",
    "\n",
    "### Data Processing\n",
    "DBLP dataset is formatted in a huge XML file which contains all paper records and metadata. It becomes impossible and super slow to parse it by loading the whole file into memory. So it has to be parsed in stream. SAX, an event-driven parser is used to parse DBLP dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBLPContentHandler(xml.sax.ContentHandler):\n",
    "    \"\"\"\n",
    "    This is a SAX XML parser for dblp.\n",
    "    Reads the dblp.xml file and produces four output files. Each file is tab-separated\n",
    "    \"\"\"\n",
    "    cur_element = -1\n",
    "    ancestor = -1\n",
    "    paper = None\n",
    "    conf = None\n",
    "    line = 0\n",
    "    errors = 0\n",
    "    author = \"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        xml.sax.ContentHandler.__init__(self)\n",
    "        DBLPContentHandler.inproc_file = open('inproc.txt', 'w')\n",
    "        DBLPContentHandler.cite_file = open('cite.txt', 'w')\n",
    "        DBLPContentHandler.conf_file = open('conf.txt', \"w\")\n",
    "        DBLPContentHandler.author_file = open(\"author.txt\", \"w\")\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == \"inproceedings\":\n",
    "            self.ancestor = ELEMENT.INPROCEEDING\n",
    "            self.cur_element = PAPER.INPROCEEDING\n",
    "            self.paper = Paper()\n",
    "            self.paper.key = attrs.getValue(\"key\")\n",
    "        elif name == \"proceedings\":\n",
    "            self.ancestor = ELEMENT.PROCEEDING\n",
    "            self.cur_element = CONFERENCE.PROCEEDING\n",
    "            self.conf = Conference()\n",
    "            self.conf.key = attrs.getValue(\"key\")\n",
    "        elif name == \"author\" and self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            self.author = \"\"\n",
    "\n",
    "        if self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            self.cur_element = PAPER.get_element(name)\n",
    "        elif self.ancestor == ELEMENT.PROCEEDING:\n",
    "            self.cur_element = CONFERENCE.get_element(name)\n",
    "        elif self.ancestor == -1:\n",
    "            self.ancestor = ELEMENT.OTHER\n",
    "            self.cur_element = ELEMENT.OTHER\n",
    "        else:\n",
    "            self.cur_element = ELEMENT.OTHER\n",
    "\n",
    "        self.line += 1\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == \"author\" and self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            self.paper.authors.append(self.author)\n",
    "\n",
    "        if ELEMENT.get_element(name) == ELEMENT.INPROCEEDING:\n",
    "            self.ancestor = -1\n",
    "            try:\n",
    "                if self.paper.title == \"\" or self.paper.conference == \"\" or self.paper.year == 0:\n",
    "                    print (\"error in parsing \" + self.paper.key)\n",
    "                    print self.paper.title\n",
    "                    print self.paper.conference\n",
    "                    print self.paper.year\n",
    "                    self.errors += 1\n",
    "                    return\n",
    "                \n",
    "                # filter only important AI conference paper\n",
    "                keywords = ['AAAI', 'CVPR', 'ICCV', 'ECCV', 'ICML', 'IJCAI', 'NIPS','ACL', \n",
    "                            'COLT', 'EMNLP', 'ECAI', 'ICRA', 'ICCBR', 'COLING', 'KR', 'UAI', \n",
    "                            'PPSN', 'ACCV', 'CoNLL', 'ICPR', 'BMVC', 'IROS', 'ACML', \n",
    "                            'SIGMOD Conference', 'SIGMOD', 'KDD', 'SIGKDD', 'SIGIR', 'VLDB', \n",
    "                            'ICDE', 'CIKM', 'PODS', 'PKDD', 'ECML/PKDD', 'ICDM', 'SDM', 'ICDT', \n",
    "                            'CIDR', 'WSDM', 'ECIR', 'PAKDD']\n",
    "                for keyword in keywords:\n",
    "                    # if keyword in self.paper.title.lower() or keyword in self.paper.conference.lower():\n",
    "                    if keyword.lower() in self.paper.conference.lower().split(\" \"):\n",
    "                        self.write_paper(self.paper)\n",
    "                        for t in self.paper.authors:\n",
    "                            self.write_author(t, self.paper)\n",
    "                        for c in self.paper.citations:\n",
    "                            if c != \"...\":\n",
    "                                self.write_citation(c, self.paper)\n",
    "                        return\n",
    "\n",
    "            except ValueError:\n",
    "                print \"error\"\n",
    "\n",
    "        elif ELEMENT.get_element(name) == ELEMENT.PROCEEDING:\n",
    "            self.ancestor = -1\n",
    "            try:\n",
    "                if self.conf.name == \"\":\n",
    "                    self.conf.name = self.conf.detail\n",
    "                if self.conf.key == \"\" or self.conf.name == \"\" or self.conf.detail == \"\":\n",
    "                    print \"no conference: line \", self.line\n",
    "                    return\n",
    "                self.write_conf(self.conf)\n",
    "            except ValueError:\n",
    "                print \"error\"\n",
    "\n",
    "    def write_conf(self, conf):\n",
    "        DBLPContentHandler.conf_file.write(conf.key + \"\\t\")\n",
    "        DBLPContentHandler.conf_file.write(conf.name + \"\\t\")\n",
    "        DBLPContentHandler.conf_file.write(conf.detail + \"\\n\")\n",
    "\n",
    "    def write_paper(self, paper):\n",
    "        # print paper.toString()\n",
    "        DBLPContentHandler.inproc_file.write(paper.title + \"\\t\")\n",
    "        DBLPContentHandler.inproc_file.write(str(paper.year) + \"\\t\")\n",
    "        DBLPContentHandler.inproc_file.write(paper.conference + \"\\t\")\n",
    "        DBLPContentHandler.inproc_file.write(paper.key + \"\\n\")\n",
    "\n",
    "    def write_author(self, t, paper):\n",
    "        DBLPContentHandler.author_file.write(t + \"\\t\")\n",
    "        DBLPContentHandler.author_file.write(paper.key + \"\\n\")\n",
    "\n",
    "    def write_citation(self, c, paper):\n",
    "        DBLPContentHandler.cite_file.write(paper.key + \"\\t\")\n",
    "        DBLPContentHandler.cite_file.write(c + \"\\n\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        content = content.encode('utf-8').replace('\\\\', '\\\\\\\\').replace('\\n', \"\").replace('\\r', \"\")\n",
    "        if self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            if self.cur_element == PAPER.AUTHOR:\n",
    "                self.author += content\n",
    "            elif self.cur_element == PAPER.CITE:\n",
    "                if len(content) == 0:\n",
    "                    return\n",
    "                self.paper.citations.append(content)\n",
    "            elif self.cur_element == PAPER.CONFERENCE:\n",
    "                self.paper.conference += content\n",
    "            elif self.cur_element == PAPER.TITLE:\n",
    "                self.paper.title += content\n",
    "            elif self.cur_element == PAPER.YEAR:\n",
    "                if len(content) == 0:\n",
    "                    return\n",
    "                try:\n",
    "                    self.paper.year = int(content)\n",
    "                except ValueError:\n",
    "                    print \"s\" + content + \"s\"\n",
    "                    print float(content)\n",
    "        elif self.ancestor == ELEMENT.PROCEEDING:\n",
    "            if self.cur_element == CONFERENCE.CONFNAME:\n",
    "                self.conf.name = content\n",
    "            elif self.cur_element == CONFERENCE.CONFDETAIL:\n",
    "                self.conf.detail = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call function to parse dblp\n",
    "source = open(\"dblp.xml\")\n",
    "xml.sax.parse(source, DBLPContentHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the raw DBLP dataset, the publication title, year, author and venue are extracted. Then we further filter the parsed data by only keeping papers from 30 hand-picked top-tier Artificial Intelligence conferences, i.e. ICML, NLPS, CVPR, SIGKDD etc. The filtered result is presented in 3 `.csv` files which include 144,051 papers and 429,247 authors from 30 conferences.\n",
    "\n",
    "### ArnetMiner Data Processing\n",
    "We used an open-source parser to parse and format the ArnetMiner dataset[3]. It had done most of the parsing jobs for us on the ArnetMiner dataset by formatting the raw dataset to multiple `.csv` files, filtering publications by the range of years and generating formatted citation graph. Upon the formatted and filtered publication data, we further filtered the publications related to Artificial Intelligence base on the title and scraped the location information of the author using the affiliation data through the Google Map API. \n",
    "\n",
    "```python\n",
    ">>> python aminer.py ParseAminerNetworkDataToCSV --local-scheduler\n",
    "```\n",
    "This will parse the raw dataset into formatted `.csv` files. \n",
    "\n",
    "Then run\n",
    "```python\n",
    ">>> python filtering.py FilterAllCSVRecordsToYearRange --start <start_year> --end <end_year> --local-scheduler\n",
    "```\n",
    ", which can filter the AI related literatures by year.\n",
    "\n",
    "Finally running\n",
    "```python \n",
    ">>> python build_graphs.py BuildAllGraphData --start <start_year> --end <end_year> --local-scheduler\n",
    "```\n",
    "to generate the formatted citation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis of Artificial Intelligence Research\n",
    "\n",
    "Artificial Intelligence is a broad research area that encompasses many topics from learning methods, robotics, vision, language to problem solving, philosophy even cognitive science. One interesting question to be investigated is how AI research evolves over time. In this section, we will present results for our temporal analysis.\n",
    "\n",
    "Firstly, we studied number of AI conference paper in DBLP each year. It is clear that AI research has increasingly gained more popularity since 1980. There is a huge jump after 2000.\n",
    "<img src=\"picture/temporal/number.png\",width=500,height=500>\n",
    "\n",
    "Then, we investigate how trending research topics change along with time. By counting bigram frequency in papers’ title (ignore stopwords), we are able to identify top 50 hot research topics of the year. Here 1985, 2000 and 2015 are selected as three representative years and their hottest research topic word-cloud are generated below. The size of the keyword is proportional to its frequency of appearing in the paper title.\n",
    "<img src=\"picture/temporal/1985.png\",width=500,height=500>\n",
    "\n",
    "In 1985, it is obvious that Expert System (logic programming, AI programs...), Natural Language (information retrieval) and Database System were three most influential subjects of the year. We did not see any current common machine learning methods here.\n",
    "<img src=\"picture/temporal/2000.png\",width=500,height=500>\n",
    "\n",
    "\n",
    "Turning to year 2000, we witnessed lots of hot research related to Vision (Robots, Object Recognition) and Data Mining. Machine learning methods such as SVM, Bayesian Net, Hidden Markov and Neural Net are now part of hot topic lists.\n",
    "<img src=\"picture/temporal/2015.png\",width=500,height=500>\n",
    "\n",
    "Then, let’s look at the past year 2015. In 2015, the most trending topics are Neural Network, Social Media and Big Data. Advanced machine learning techniques such as deep learning, CNN, recurrent neural began to appear in hot topics. It is also worth noticing that with extremely large data volume generated in current age, research topics such big data, dictionary learning and matrix factorization become much more popular these days.\n",
    "\n",
    "According to the popularity investigated above, several topics are selected to examine their popularity change over time. As example, 3 topics which were among the most popular topics of AI during years, are picked and discussed below. \n",
    "\n",
    "Expert system was extremely popular around 1985s. However, In the 1990s and beyond, the term expert system mostly dropped from the IT lexicon. This is due to \"expert systems failed”: the IT world moved on because expert systems didn't deliver on their over hyped promise[4].\n",
    "<img src=\"picture/temporal/expertsystem.png\",width=500,height=500>\n",
    "\n",
    "Data mining is a new field that gained popular around 1990s. Since then, it continues to be popular even until now.\n",
    "<img src=\"picture/temporal/mining.png\",width=500,height=500>\n",
    "\n",
    "Convolutional neural network has an interesting history of development. It is shown that convolutional network has been applied since early 1990, yet it has not become very popular until 2010 when it proved to be working extremely effective in vision research. So we observed an extreme surge around 2010.\n",
    "<img src=\"picture/temporal/cnn.png\",width=500,height=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Analysis of Artificial Intelligence Research Community\n",
    "\n",
    "Another question we want to answer is how AI research popularity differs from place to place. So next, we will further investigate the geographic distribution of machine learning research. We use ArnetMiner dataset which contains author’s affiliation. By using Google Map API we are able to scrape the latitude and longitude location for each author. Then Map Data website http://www.mapsdata.co.uk/about is used to create a heat map for AI research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geocoder\n",
    "import requests\n",
    "\n",
    "# short example of how to get coordinate of an address\n",
    "g = geocoder.google(\"Carnegie Mellon University\")\n",
    "print (str(g.latlng[0]) + \"\\t\" + str(g.latlng[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at current AI research distribution around the world. The figure shows the number of researchers located in each area. We found that North America, Europe and Asia are three largest current AI research community. We can discovered that AI research has been grown aggressively in recent years around China, India, Iran, Singapore and Taiwan.\n",
    "\n",
    "<img src=\"picture/geo/world.png\",width=800,height=800>\n",
    "\n",
    "These figures below show detailed distribution within different continents. From these figures we are able to identify huge clusters around Spain, France, Germany, Italy, China, Japan, South Korea. There are also small clusters of authors distributed in Canada, India, Brazil, India, Iran, and Turkey.\n",
    "\n",
    "<img src=\"picture/geo/combination.png\",width=900,height=900>\n",
    "\n",
    "By observing the figure, we found there is a strong correlation between the AI research popularity and economic prosperity. Most of the authors are distributed in the developed countries or developing countries with high investment of research such as China and India. It is sad to see there is currently very few research in north Africa and middle east."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Finding within Coauthorship and Citation Network\n",
    "\n",
    "To fully understand a research field, it is important to look at relationship between researchers. Coauthorship and Citation networks provide an easy way to address many of these questions. In this session, firstly, we made a simple statistic analysis on the network to identify the most-cited authors. Next, we implemented fast unfolding algorithm to identify different research communities within the graph. Fast unfolding of communities in large networks is an efficient algorithm used to extract network structure. We visualized the citation and coauthor graph using open-source software Gephi. Through the visualization, we are able to identify important research communities, research fields and leading researcher within each field.\n",
    "\n",
    "Firstly, by doing simple degree count on the graph, we were able to identify the most cited authors in recent years (20101-2016). Chih-Jen Lin and Chih-Chung Chang are ranked on top because of their work of LIBVIM in 2011. Jiawei’s work in data mining has profound influence on the field which makes him ranked at the 3rd.\n",
    "\n",
    "<img src=\"picture/graph/table.png\",width=600>\n",
    "\n",
    "Next, we tried to do community finding on coauthorship and citation networks. We implemented the fast unfolding algorithm to extract the community structure. Here we will briefly introduce the algorithm.\n",
    "\n",
    "Fast unfolding algorithm is based on modularity optimization, which has considered the tradeoff between runtime and accuracy. There are two passes in this algorithm. We add an edge for each pair of people who are the coauthor in the same paper, and then we build an undirected graph. Then we run the fast unfolding algorithm on this graph: For each node in the first pass, we try to find the best adjacent cluster, based on the highest modularity increments:\n",
    "\n",
    "$$\\Delta Q= \\left[\\frac{\\sum_{in} + k_{i,in}}{2m} - \\left(\\frac{\\sum_{tot} + k_i}{2m}\\right)^2\\right] - \\left[\\frac{\\sum_{in}}{2m} - \\left(\\frac{\\sum_{tot}}{2m}\\right)^2 - \\left(\\frac{k_i}{2m}\\right)^2\\right]$$\n",
    "\n",
    "And then in the second pass, we will merge the same cluster as one single point, regard all the inner connection as one self connection with weight, and merge all the outer connection. This helps us to identify communities in a hierarchical way. Run this graph from the first pass, then we will have a compact graph in the end. Fast unfolding proved to be an efficient way to partition network into different communities[6].\n",
    "\n",
    "For efficiency, the algorithm above is implemented in C++. So it is not included in the report. For further detail, please refer to `code/cluster_pipeline/`.\n",
    "\n",
    "\n",
    "## Coauthorship Network\n",
    "Coauthorship network provides rich information in research collaboration. By running fast unfolding community-finding algorithm on the graph, we are able to form collaboration relationship between authors therefore identify important research groups. Our coauthor network is generated by manually parsing DBLP dataset. The initial network contains 540k authors. We firstly filtered out authors who has less than 50 publications which left only 3220 nodes. Our algorithm were able to detect 73 communities within them. We selected top 100 authors and visualized our communities using Gephi software. Each color represents a research community where research collaboration is intense.\n",
    "\n",
    "![co-author graph](picture/graph/coauthor.png)\n",
    "\n",
    "As we can see, the network splits into many communities, many of which are groups with special characteristics. On the top we have a large  “data mining” group lead by Jiawei Chen and Phillip S Yu. The group also contains researchers outside US probably because their rich connection to Asia institutes. On the left, we have “computer vision” group lead by Tomas S Huang and Shuicheng Yan.  On the right, we have “Database & Graph Mining” group lead by Christos Faloutsos and Raghu Ramakrishnan. We also see some small groups at bottom such as “AI robotics” group of Sebastian Thrun and Wolfram Burgard, “learning & vision” group by Eric Xing, Fei-Fei Li and Michael Jordan.\n",
    "\n",
    "From the graph, we can tell several important observations. Data mining and computer vision are two of the most active research fields with largest research communities. Secondly, there are increasingly cross-regional research collaboration between different institute. For example, Jiawei Han is a professor located at UIUC yet his coauthor network has people from China, Singapore and UK. Thirdly, although we manually splits the network into different communities, they are not isolated from each other. Many cross-communities collaboration could be found within the network which demonstrates that researchers are working closely with each other.\n",
    "\n",
    "## Citation Network\n",
    "Besides coauthorship, we also study the cluster within the citation graph. Coauthor network allows us to identify different research communities while citation  network allows us to identify different research field and leading researchers within each field. We use ArnetMiner dataset for citation information which contains 28526 researchers and 191,128 citation links of AI research from 2010-2016.\n",
    "\n",
    "By running algorithm of this paper, we can partition the graph into 180 communities. We visualized top 100 most cited authors in the following figure. On the left, the blue and pink clusters are for machine learning and computer vision research. On the right, we have several small green and orange clusters for data mining. On the bottom, the dark red nodes represents NLP research communities.\n",
    "\n",
    "![citation graph](picture/graph/citation.png)\n",
    "\n",
    "We were able to make several conclusions from this graph. From the number of citation, we can tell that from 2010-2016, computer vision research has more citations than other research which indicates the popularity for the field. In the computer vision research field, based on citation analysis, it seems L.Van Gool, Li Fei-Fei, P.H.S. Torr are important researchers in the field. (The size of the node indicates its pagerank score.) For data mining field, the leading researchers are Jiawei Han and Nitesh V. Chawla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence Literature Recommendation System\n",
    "\n",
    "From our own experience of doing researches and writing research literatures on unfamiliar topics, it is always the case that we left some important papers out when there is no one instructing us. In this scenario, it would be extremely helpful if there is a system giving recommendations on related paper that worth reading. Therefore, at last but not least, we designed and implemented a simple literature recommendation system. Given a research paper the user is focusing on, the system can recommend the most related and influential paper to the user.\n",
    "\n",
    "\n",
    "The recommendation problem is simply modeled by the Bayesian equation:\n",
    "$$P(l_i|l_q) = \\frac{P(l_q|l_i)\\cdot P(l_i)}{P(l_q)}$$\n",
    "Where $l_i$, $l_q$ separately denote the candidate literature and query literature. Since $P(l_q)$ is a constant to all $l_i$, we can further deduct the equation to:\n",
    "$$P(l_i|l_q) \\propto P(l_q|l_i)\\cdot P(l_i)$$\n",
    "For the problem solving, we define the likelihood $P(l_q|l_i)$ to be the topic similarity between $l_i$ and $l_q$ and the prior $P(l_i)$ to be the number of appearances of $l_i$ near by $l_q$ in the citation graph $G_c$ (to speed up the calculation).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "from collections import Counter\n",
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from Queue import Queue\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For topic similarity, we separately calculated the similarity of the two publications’ titles and abstracts. \n",
    "\n",
    "\n",
    "The title similarity is defined as the number of same bigram appeared in the two titles. Here, we use the k-gram index method[5] to firstly build a hashtable with the key of every bigram ever appeared in every title and value of a list of title indexes that contain this bigram. Then given a query title, the similarity calculation can be simplified to merging the lists found by the bigrams appeared in the query title. The score $S_t$ of title similarity will be the number of appearances of the titles in the lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_add(bigram_dict, key, value):\n",
    "    if key not in bigram_dict:\n",
    "        bigram_dict[key] = set()\n",
    "    bigram_dict[key].add(value)\n",
    "\n",
    "\n",
    "def bigram_index(titles):\n",
    "    bigram = dict()\n",
    "\n",
    "    for key, title in titles.iteritems():\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        try:\n",
    "            words = [w.lower() for w in title.split(' ') if w not in stop_words]\n",
    "\n",
    "            if len(words) < 2:\n",
    "                bigram_add(bigram, words[0], key)\n",
    "                continue\n",
    "            for i in range(len(words) - 1):\n",
    "                bi = words[i: i + 2]\n",
    "                bigram_add(bigram, ' '.join(bi), key)\n",
    "        except:\n",
    "            print 'exception:', key, title\n",
    "            continue\n",
    "    return bigram\n",
    "\n",
    "\n",
    "def bigram_search(bigram_dict, title):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w.lower() for w in title.split(' ') if w not in stop_words]\n",
    "\n",
    "    bi_list = list()\n",
    "    for i in range(len(words) - 1):\n",
    "        bi = ' '.join(words[i: i + 2])\n",
    "        if bi in bigram_dict:\n",
    "            bi_list += list(bigram_dict[bi])\n",
    "    bi_cnt = Counter(bi_list)\n",
    "    retval = dict()\n",
    "    for w in bi_cnt:\n",
    "        if bi_cnt[w] >= 2:\n",
    "            retval[w] = bi_cnt[w]\n",
    "\n",
    "    # normalization\n",
    "    max_val = max(retval.values())\n",
    "    for w in retval:\n",
    "        retval[w] = float(retval[w]) / max_val\n",
    "    return retval\n",
    "\n",
    "\n",
    "def build_bigram_index():\n",
    "    paper_df = pd.read_csv(os.path.join(config.base_csv_dir, 'paper.csv'))\n",
    "    title_dict = dict()\n",
    "    for index, row in paper_df.iterrows():\n",
    "        title_dict[int(row['id'])] = row['title']\n",
    "\n",
    "    print type(title_dict)\n",
    "    print title_dict.keys()[0], title_dict[title_dict.keys()[0]]\n",
    "\n",
    "    bigram_dict = bigram_index(title_dict)\n",
    "    save_obj(bigram_dict, 'bigram_idx')\n",
    "\n",
    "    \n",
    "def load_bigram_dict():\n",
    "    if not os.path.exists('./bigram_idx.pkl'):\n",
    "        build_bigram_index()\n",
    "    return load_obj('bigram_idx')\n",
    "\n",
    "\n",
    "def load_paper_dict():\n",
    "    if not os.path.exists('./paper_dict.pkl'):\n",
    "        paper_dict = dict()\n",
    "        paper_df = pd.read_csv(os.path.join(config.base_csv_dir, 'paper.csv'))\n",
    "        for _, row in paper_df.iterrows():\n",
    "            pid = int(row['id'])\n",
    "            paper_dict[pid] = dict()\n",
    "            paper_dict[pid]['title'] = row['title']\n",
    "            paper_dict[pid]['abstract'] = row['abstract']\n",
    "        save_obj(paper_dict, 'paper_dict')\n",
    "    return load_obj('paper_dict')\n",
    "\n",
    "\n",
    "def load_paper_inv_dict():\n",
    "    if not os.path.exists('./paper_inv_dict.pkl'):\n",
    "        paper_inv_dict = dict()\n",
    "        paper_df = pd.read_csv(os.path.join(config.base_csv_dir, 'paper.csv'))\n",
    "        for _, row in paper_df.iterrows():\n",
    "            pid = int(row['id'])\n",
    "            title = row['title']\n",
    "            paper_inv_dict[title] = pid\n",
    "        save_obj(paper_inv_dict, 'paper_inv_dict')\n",
    "    return load_obj('paper_inv_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract similarity is calculated using the popular technique in search engine - Indri. For each candidate’s abstract $d$, we calculate the score $S_a$ with the query abstract $q$ using the following equation:\n",
    "$$ S_a(l) = P(q|d) = \\sum_{i} p(q_i|d) = \\sum_{i} (1-\\lambda )\\frac{tf_{q_i,d} + \\mu p_{MLE}(q_i|C)}{length(d) + \\mu} + \\lambda p_{MLE}(q_i|C) $$\n",
    "Where $\\lambda$ and $\\mu$ are parameters for smoothing, we take 0.1 here. $tf_{q_i,d}$ is the term frequency of query term in the doc, $p_{MLE}(q_i|C)$ is calculated as corpus term frequency of query term.\n",
    "\n",
    "Then the likelihood $P(l_q|l_i)$ is calculated as:\n",
    "$$P(l_q|l_i) = 0.5 \\cdot (normalized(S_t(l)) + normalized(S_a(l)) )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class abstract_similarity:\n",
    "    lam = 0.1\n",
    "    mu = 0.1\n",
    "\n",
    "    def __init__(self, path=os.path.join(config.filtered_dir, 'paper-2010-2016.csv')):\n",
    "        file = open(path)\n",
    "        stop = set(stopwords.words('english'))\n",
    "        stop.add(\"using\")\n",
    "        stop.add(\"based\")\n",
    "\n",
    "        self.dict = {}\n",
    "        self.total = 0\n",
    "        for line in file:\n",
    "            line = line.rstrip().lower()\n",
    "            line = line[line.find('\"'):]\n",
    "            title = line.lower().rstrip().translate(None, string.punctuation)\n",
    "            words = title.split(\" \")\n",
    "            for i in range(0, len(words)):\n",
    "                word = words[i]\n",
    "                if words[i] in stop :\n",
    "                    continue\n",
    "                if word in self.dict:\n",
    "                    self.dict[word] += 1\n",
    "                else:\n",
    "                    self.dict[word] = 1\n",
    "                self.total += 1\n",
    "        file.close()\n",
    "\n",
    "    def getScore(self, abstract1, abstract2):\n",
    "        '''\n",
    "        :param abstract1: input\n",
    "        :param abstract2: another doc that needs to be compared\n",
    "        :return: similarity score\n",
    "        '''\n",
    "        counter2 = {}\n",
    "        stop = set(stopwords.words('english'))\n",
    "        words1 = abstract1.rstrip().lower().rstrip().translate(None, string.punctuation).split(\" \")\n",
    "        words2 = abstract2.rstrip().lower().rstrip().translate(None, string.punctuation).split(\" \")\n",
    "        stop.add(\"using\")\n",
    "        stop.add(\"based\")\n",
    "        doclen = 0\n",
    "        for word in words2:\n",
    "            if word in stop:\n",
    "                continue\n",
    "            doclen += 1\n",
    "            if word in counter2:\n",
    "                counter2[word] += 1\n",
    "            else:\n",
    "                counter2[word] = 1\n",
    "\n",
    "        score = 0.0\n",
    "        for word in words1:\n",
    "            tf = 0.0\n",
    "            if word in stop:\n",
    "                continue\n",
    "            if word in counter2:\n",
    "                tf = counter2[word]\n",
    "            if word in self.dict:\n",
    "                ctf = self.dict[word]\n",
    "            else:\n",
    "                ctf = 0.0\n",
    "            score += (1-self.lam) * ((tf + self.mu * ctf / self.total) / (doclen + self.mu)) + self.lam * (ctf / self.total)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reference score $P(l_i)$, we simply count the appearance number of the papers which are within distance 3 from the query paper in the citation graph. So that this score will capture the popularity among the papers in the small cluster of papers around the query paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_ref_dict():\n",
    "    print 'building'\n",
    "    ref_df = pd.read_csv(os.path.join(config.base_csv_dir, 'refs.csv'))\n",
    "    ref_dict = dict()\n",
    "    for _, row in ref_df.iterrows():\n",
    "        pid = int(row['paper_id'])\n",
    "        if pid not in ref_dict:\n",
    "            ref_dict[pid] = list()\n",
    "        ref_dict[pid].append(int(row['ref_id']))\n",
    "    save_obj(ref_dict, 'ref_idx')\n",
    "\n",
    "\n",
    "def load_ref_dict():\n",
    "    if not os.path.exists('ref_idx.pkl'):\n",
    "        build_ref_dict()\n",
    "\n",
    "    return load_obj('ref_idx')\n",
    "\n",
    "\n",
    "def graph_count(ref_idx, paper_id, niter=3):\n",
    "    vertices = list()\n",
    "    queue = Queue()\n",
    "    queue.put(paper_id)\n",
    "    for i in range(niter):\n",
    "        n = queue.qsize()\n",
    "        for j in range(n):\n",
    "            pid = queue.get()\n",
    "            try:\n",
    "                citations = ref_idx[pid]\n",
    "                for cite in citations:\n",
    "                    queue.put(cite)\n",
    "                    vertices.append(cite)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    retval = dict()\n",
    "    vertices_cnt = Counter(vertices)\n",
    "    for pid in vertices_cnt:\n",
    "        retval[pid] = vertices_cnt[pid]\n",
    "\n",
    "    # normalization\n",
    "    max_val = max(retval.values())\n",
    "    for w in retval:\n",
    "        retval[w] = float(retval[w]) / max_val\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system will then return the top 5 scored candidates as the recommendation paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import title_similarity\n",
    "import ref_score\n",
    "import similarity\n",
    "import operator\n",
    "\n",
    "class recommendation:\n",
    "    def __init__(self):\n",
    "        # set up citation reference score index\n",
    "        self.ref_dict = ref_score.load_ref_dict()\n",
    "        print 'index loading done'\n",
    "\n",
    "        # set up title bigram feature\n",
    "        self.bigram_dict = title_similarity.load_bigram_dict()\n",
    "        print 'bigram load done'\n",
    "\n",
    "        self.paper_dict = title_similarity.load_paper_dict()\n",
    "        self.paper_inv_dict = title_similarity.load_paper_inv_dict()\n",
    "        print 'paper load done'\n",
    "\n",
    "        # set up abstract similarity feature\n",
    "        self.simi_eva = similarity.abstract_similarity()\n",
    "        print \"abstract index done\"\n",
    "\n",
    "\n",
    "    def get(self, paper_title):\n",
    "        if paper_title not in self.paper_inv_dict:\n",
    "            return list()\n",
    "        pid = self.paper_inv_dict[paper_title]\n",
    "        title = self.paper_dict[pid]['title']\n",
    "        abstract = self.paper_dict[pid]['abstract']\n",
    "\n",
    "        # calculate title bigram feature\n",
    "        idx_list = title_similarity.bigram_search(self.bigram_dict, title)\n",
    "\n",
    "        # calculate citation feature\n",
    "        cnt = ref_score.graph_count(self.ref_dict, pid, 3)\n",
    "\n",
    "        # calculate prior\n",
    "        candidates = {}\n",
    "        for k, v in idx_list.iteritems():\n",
    "            if k in candidates:\n",
    "                candidates[k] += v\n",
    "            else:\n",
    "                candidates[k] = v\n",
    "\n",
    "        for k, v in cnt.iteritems():\n",
    "            if k in candidates:\n",
    "                candidates[k] += v\n",
    "            else:\n",
    "                candidates[k] = v\n",
    "\n",
    "        # calculate total score\n",
    "        for id, prior in candidates.iteritems():\n",
    "            if id in self.paper_dict and type(self.paper_dict[id]['abstract']) == str:\n",
    "                score = prior * self.simi_eva.getScore(abstract1=abstract, abstract2=self.paper_dict[id]['abstract'])\n",
    "            else:\n",
    "                score = prior * 0.5\n",
    "            candidates[id] = score\n",
    "\n",
    "        sort_score = sorted(candidates.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        retval = list()\n",
    "        for item in sort_score:\n",
    "            try:\n",
    "                title = self.paper_dict[item[0]]['title']\n",
    "                retval.append(title)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        return retval[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building\n",
      "index loading done\n",
      "<type 'dict'>\n",
      "1747627 Sequential feature selection for classification\n",
      "exception: 1446891 nan\n",
      "exception: 1309608 nan\n",
      "bigram load done\n",
      "paper load done\n",
      "abstract index done\n",
      "['K-AP Clustering Algorithm for Large Scale Dataset', 'A New Clustering Algorithm of Large Datasets with O(N) Computational Complexity', 'Fast discovery of association rules', 'An Efficient Density Based Clustering Algorithm for Large Databases', 'An effective hash-based algorithm for mining association rules']\n"
     ]
    }
   ],
   "source": [
    "re = recommendation()\n",
    "print re.get(\"New unsupervised clustering algorithm for large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the query above, our implementation returns:\n",
    "```python\n",
    "['K-AP Clustering Algorithm for Large Scale Dataset', 'A New Clustering Algorithm of Large Datasets with O(N) Computational Complexity', 'Fast discovery of association rules', 'An Efficient Density Based Clustering Algorithm for Large Databases', 'An effective hash-based algorithm for mining association rules']\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] http://dblp.uni-trier.de/\n",
    "\n",
    "[2] https://aminer.org/billboard/aminernetwork\n",
    "\n",
    "[3] https://github.com/macks22/dblp\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Expert_system\n",
    "\n",
    "[5] http://nlp.stanford.edu/IR-book/html/htmledition/k-gram-indexes-for-spelling-correction-1.html\n",
    "\n",
    "[6] Blondel, Vincent D., et al. \"Fast unfolding of communities in large networks.\" Journal of statistical mechanics: theory and experiment 2008.10 (2008): P10008.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
