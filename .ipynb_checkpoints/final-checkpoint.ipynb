{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Community: Social Discovery of Artificial Intelligence Researchers and Literatures\n",
    "\n",
    "Artificial Intelligence is one of the most influential subjects in computer science. We will analyze scholars’ activities based on literatures published in the top-tier conferences to gain some interesting insights into this field. \n",
    "\n",
    "In this report, we start by a brief introduction of the dataset and data processing. Then we present a temporal analysis of AI topic popularity. In section 3, spatial distribution of AI research is discussed. Next, in section 4, we implement community finding algorithm on coauthorship and citation graphs, which identifies important research communities and centers within the graph. Finally, we build a simple paper recommendation system based on content similarity and citation relationship. \n",
    "\n",
    "Note: The code cannot be run directly from this Python Notebook. Because our code is too long to be included. We have included a seperate folder for code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection and Processing\n",
    "For this project, our data mainly came from two public dataset, DBLP and ArnetMiner. Apart from them, we also scraped data from Google Map for geographical information. In this section, we will discuss how we parsed them and how we used them.\n",
    "\n",
    "### Data Selection\n",
    "DBLP is a well-maintained dataset of computer science bibliography. It contains 3,583,224 publications and 1,824,011 authors[1]. It covers a huge amount of computer science publications. Taking advantage of the widely covered publications, DBLP is used for analysing the trending of topics. We also gathered and formatted the co-authorship information for the analysis on community finding. However, DBLP has two obvious drawbacks. DBLP does not have well maintained author information and it does not have the paper citation data. It causes great inconvenience for our further analysis so we turned to a second dataset ArnetMiner.\n",
    "\n",
    "The other public dataset we use is ArnetMiner[2], which is a supplement to DBLP that has well-maintained metadata including citations and author metadata. ArnetMiner contains 2,092,356 publications, 1,712,433 authors and 8,024,869 citation relationships[2]. Thus, we used the ArnetMiner dataset for community finding based on citation relationships. For the analysis of authors’ geographical distribution, we also scrape the geographical coordinates of the authors’ affiliations to locate each author.\n",
    "\n",
    "### Data Processing\n",
    "DBLP dataset is formatted in a huge XML file which contains all paper records and metadata. It become impossible and super slow to parse it by loading the whole file to memory. So it become a requirement to parse the dataset in stream. SAX, an event-driven parser is used to parse DBLP dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBLPContentHandler(xml.sax.ContentHandler):\n",
    "    \"\"\"\n",
    "    This is a SAX XML parser for dblp.\n",
    "    Reads the dblp.xml file and produces four output files. Each file is tab-separated\n",
    "    \"\"\"\n",
    "    cur_element = -1\n",
    "    ancestor = -1\n",
    "    paper = None\n",
    "    conf = None\n",
    "    line = 0\n",
    "    errors = 0\n",
    "    author = \"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        xml.sax.ContentHandler.__init__(self)\n",
    "        DBLPContentHandler.inproc_file = open('inproc.txt', 'w')\n",
    "        DBLPContentHandler.cite_file = open('cite.txt', 'w')\n",
    "        DBLPContentHandler.conf_file = open('conf.txt', \"w\")\n",
    "        DBLPContentHandler.author_file = open(\"author.txt\", \"w\")\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == \"inproceedings\":\n",
    "            self.ancestor = ELEMENT.INPROCEEDING\n",
    "            self.cur_element = PAPER.INPROCEEDING\n",
    "            self.paper = Paper()\n",
    "            self.paper.key = attrs.getValue(\"key\")\n",
    "        elif name == \"proceedings\":\n",
    "            self.ancestor = ELEMENT.PROCEEDING\n",
    "            self.cur_element = CONFERENCE.PROCEEDING\n",
    "            self.conf = Conference()\n",
    "            self.conf.key = attrs.getValue(\"key\")\n",
    "        elif name == \"author\" and self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            self.author = \"\"\n",
    "\n",
    "        if self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            self.cur_element = PAPER.get_element(name)\n",
    "        elif self.ancestor == ELEMENT.PROCEEDING:\n",
    "            self.cur_element = CONFERENCE.get_element(name)\n",
    "        elif self.ancestor == -1:\n",
    "            self.ancestor = ELEMENT.OTHER\n",
    "            self.cur_element = ELEMENT.OTHER\n",
    "        else:\n",
    "            self.cur_element = ELEMENT.OTHER\n",
    "\n",
    "        self.line += 1\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == \"author\" and self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            self.paper.authors.append(self.author)\n",
    "\n",
    "        if ELEMENT.get_element(name) == ELEMENT.INPROCEEDING:\n",
    "            self.ancestor = -1\n",
    "            try:\n",
    "                if self.paper.title == \"\" or self.paper.conference == \"\" or self.paper.year == 0:\n",
    "                    print (\"error in parsing \" + self.paper.key)\n",
    "                    print self.paper.title\n",
    "                    print self.paper.conference\n",
    "                    print self.paper.year\n",
    "                    self.errors += 1\n",
    "                    return\n",
    "                \n",
    "                # filter only important AI conference paper\n",
    "                keywords = ['AAAI', 'CVPR', 'ICCV', 'ECCV', 'ICML', 'IJCAI', 'NIPS', 'ACL', 'COLT', 'EMNLP', 'ECAI', 'ICRA', 'ICCBR', 'COLING', 'KR', 'UAI', 'PPSN', 'ACCV', 'CoNLL', 'ICPR', 'BMVC', 'IROS', 'ACML', 'SIGMOD Conference', 'SIGMOD', 'KDD', 'SIGKDD', 'SIGIR', 'VLDB', 'ICDE', 'CIKM', 'PODS', 'PKDD', 'ECML/PKDD', 'ICDM', 'SDM', 'ICDT', 'CIDR', 'WSDM', 'ECIR', 'PAKDD']\n",
    "                for keyword in keywords:\n",
    "                    # if keyword in self.paper.title.lower() or keyword in self.paper.conference.lower():\n",
    "                    if keyword.lower() in self.paper.conference.lower().split(\" \"):\n",
    "                        self.write_paper(self.paper)\n",
    "                        for t in self.paper.authors:\n",
    "                            self.write_author(t, self.paper)\n",
    "                        for c in self.paper.citations:\n",
    "                            if c != \"...\":\n",
    "                                self.write_citation(c, self.paper)\n",
    "                        return\n",
    "\n",
    "            except ValueError:\n",
    "                print \"error\"\n",
    "\n",
    "        elif ELEMENT.get_element(name) == ELEMENT.PROCEEDING:\n",
    "            self.ancestor = -1\n",
    "            try:\n",
    "                if self.conf.name == \"\":\n",
    "                    self.conf.name = self.conf.detail\n",
    "                if self.conf.key == \"\" or self.conf.name == \"\" or self.conf.detail == \"\":\n",
    "                    print \"no conference: line \", self.line\n",
    "                    return\n",
    "                self.write_conf(self.conf)\n",
    "            except ValueError:\n",
    "                print \"error\"\n",
    "\n",
    "    def write_conf(self, conf):\n",
    "        DBLPContentHandler.conf_file.write(conf.key + \"\\t\")\n",
    "        DBLPContentHandler.conf_file.write(conf.name + \"\\t\")\n",
    "        DBLPContentHandler.conf_file.write(conf.detail + \"\\n\")\n",
    "\n",
    "    def write_paper(self, paper):\n",
    "        # print paper.toString()\n",
    "        DBLPContentHandler.inproc_file.write(paper.title + \"\\t\")\n",
    "        DBLPContentHandler.inproc_file.write(str(paper.year) + \"\\t\")\n",
    "        DBLPContentHandler.inproc_file.write(paper.conference + \"\\t\")\n",
    "        DBLPContentHandler.inproc_file.write(paper.key + \"\\n\")\n",
    "\n",
    "    def write_author(self, t, paper):\n",
    "        DBLPContentHandler.author_file.write(t + \"\\t\")\n",
    "        DBLPContentHandler.author_file.write(paper.key + \"\\n\")\n",
    "\n",
    "    def write_citation(self, c, paper):\n",
    "        DBLPContentHandler.cite_file.write(paper.key + \"\\t\")\n",
    "        DBLPContentHandler.cite_file.write(c + \"\\n\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        content = content.encode('utf-8').replace('\\\\', '\\\\\\\\').replace('\\n', \"\").replace('\\r', \"\")\n",
    "        if self.ancestor == ELEMENT.INPROCEEDING:\n",
    "            if self.cur_element == PAPER.AUTHOR:\n",
    "                self.author += content\n",
    "            elif self.cur_element == PAPER.CITE:\n",
    "                if len(content) == 0:\n",
    "                    return\n",
    "                self.paper.citations.append(content)\n",
    "            elif self.cur_element == PAPER.CONFERENCE:\n",
    "                self.paper.conference += content\n",
    "            elif self.cur_element == PAPER.TITLE:\n",
    "                self.paper.title += content\n",
    "            elif self.cur_element == PAPER.YEAR:\n",
    "                if len(content) == 0:\n",
    "                    return\n",
    "                try:\n",
    "                    self.paper.year = int(content)\n",
    "                except ValueError:\n",
    "                    print \"s\" + content + \"s\"\n",
    "                    print float(content)\n",
    "        elif self.ancestor == ELEMENT.PROCEEDING:\n",
    "            if self.cur_element == CONFERENCE.CONFNAME:\n",
    "                self.conf.name = content\n",
    "            elif self.cur_element == CONFERENCE.CONFDETAIL:\n",
    "                self.conf.detail = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to parse dblp\n",
    "source = open(\"dblp.xml\")\n",
    "xml.sax.parse(source, DBLPContentHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the raw DBLP dataset, the publication title, year, author and venue are extracted. Then we further filter the parsed data by only keeping papers from 30 hand-picked top-tier Artificial Intelligence conferences, i.e. ICML, NLPS, CVPR, SIGKDD etc. The filtered result is presented in 3 `.csv` files which include 144,051 papers and 429,247 authors from 30 conferences.\n",
    "\n",
    "### ArnetMiner Data Processing\n",
    "We used an open-source parser to parse and format the ArnetMiner dataset[3]. It had done most of the parsing jobs for us on the ArnetMiner dataset by formatting the raw dataset to multiple `.csv` files like we did on parsing DBLP, filtering publications by the range of years and generating formatted citation graph. On the formatted and filtered publications by the parser, we further filtered the publications related to Artificial Intelligence base on the title of the publication and fulfilled the location information of the author’s using the affiliation data through the Google Map API. \n",
    "\n",
    "```python\n",
    ">>> python aminer.py ParseAminerNetworkDataToCSV --local-scheduler\n",
    "```\n",
    "This will parse the raw dataset into formatted `.csv` files. \n",
    "\n",
    "Then run\n",
    "```python\n",
    ">>> python filtering.py FilterAllCSVRecordsToYearRange --start <start_year> --end <end_year> --local-scheduler\n",
    "```\n",
    ", which can filter the AI related literatures by year.\n",
    "\n",
    "Finally running\n",
    "```python \n",
    "python build_graphs.py BuildAllGraphData --start <start_year> --end <end_year> --local-scheduler\n",
    "```\n",
    "to generate the formatted citation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis of Artificial Intelligence Research\n",
    "\n",
    "Artificial Intelligence is a broad research area that encompasses many topics from learning methods, robotics, vision, language to problem solving, philosophy even cognitive science. One interesting question to be investigated is how AI research evolve over time. In this section, we will present results for our temporal analysis.\n",
    "\n",
    "Firstly, we studied number of AI conference paper in DBLP each year. It is clear that AI research has increasingly gained more popularity since 1980. There is a huge jump after 2000.\n",
    "\n",
    "``` TODO: \n",
    "insert image <AI Conference Paper Published for Each Year\n",
    "```\n",
    "Then, we investigate how trending research topics change along with time. By counting bigram frequency in papers’ title (ignore stopwords), we are able to identify top 50 hot research topics of the year. Here 1985, 2000 and 2015 are selected as three representative years and their hottest research topic word-cloud are generated below. The size of the keyword is proportional to its frequency of appearing in the paper title.\n",
    "\n",
    "``` TODO:\n",
    "insert image <word cloud 1985>\n",
    "```\n",
    "In 1985, it is obvious that Expert System (logic programming, ai programs...), Natural Language (information retrieval) and Database System were three most influential subjects of the year. We did not see any current common machine learning methods here.\n",
    "``` TODO:\n",
    "insert image <word cloud 2000>\n",
    "```\n",
    "Turning to year 2000, we witnessed lots of hot research related to Vision (Robots, Object Recognition) and Data Mining. Machine learning methods such as SVM, Bayesian Net, Hidden Markov and Neural Net are now part of hot topic lists.\n",
    "``` TODO:\n",
    "insert image <word cloud 2015>\n",
    "```\n",
    "Then, let’s look at the past year 2015. In 2015, the most trending topics are Neural Network, Social Media and Big Data. Advanced machine learning techniques such as deep learning, CNN, recurrent neural began to appear in hot topics. It is also worth noticing that with extremely large data volume generated in current age, research topics such big data, dictionary learning and matrix factorization become much more popular these days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence Literature Recommendation System\n",
    "\n",
    "From our own experience of doing researches and writing research literatures on unfamiliar topics, it is always the case that we left some important papers out when there is no one instructing us. In this scenario, it would be extremely helpful if there is a system giving recommendations on related paper that worth reading. Therefore, at last but not least, we designed and implemented a simple literature recommendation system. Given a research paper the user is focusing on, the system can recommend the most related and influential paper to the user.\n",
    "\n",
    "\n",
    "The recommendation problem is simply modeled by the Bayesian equation:\n",
    "$$P(l_i|l_q) = \\frac{P(l_q|l_i)\\cdot P(l_i)}{P(l_q)}$$\n",
    "Where $l_i$, $l_q$ separately denote the candidate literature and query literature. Since $P(l_q)$ is a constant to all $l_i$, we can further deduct the equation to:\n",
    "$$P(l_i|l_q) \\propto P(l_q|l_i)\\cdot P(l_i)$$\n",
    "For the problem solving, we define the likelihood $P(l_q|l_i)$ to be the topic similarity between $l_i$ and $l_q$ and the prior $P(l_i)$ to be the number of appearances of $l_i$ near by $l_q$ in the citation graph $G_c$ (to speed up the calculation).  \n",
    "\n",
    "\n",
    "For topic similarity, we separately calculated the similarity of the two publications’ titles and abstracts. The title similarity is defined as the number of same bigram appeared in the two titles. And we calculate the abstract similarity by using the popular technique in search engine - Indri.\n",
    "\n",
    "\n",
    "For the reference score, we simply count the appearance number of the papers which are within distance 2 from the query paper in the citation graph. So that this score will capture the popularity among the papers in  the small cluster of papers around the  query paper.\n",
    "\n",
    "\n",
    "Finally, we define the score of the candidate paper as:\n",
    "$S = S_{topic} + S_{reference}$\n",
    "The system will then return the top 5 scored candidates as the recommendation paper.\n",
    "\n",
    "\n",
    "`code` and explain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] http://dblp.uni-trier.de/\n",
    "\n",
    "[2] https://aminer.org/billboard/aminernetwork\n",
    "\n",
    "[3] https://github.com/macks22/dblp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
